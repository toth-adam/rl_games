{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from windy import WindyGridWorld\n",
    "import numpy as np\n",
    "from collections import deque\n",
    "import pickle\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Necessary functions "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def epsilon_greedy_policy(q_vector, epsilon):\n",
    "    is_greedy_action = False if np.random.uniform() <= epsilon else True\n",
    "    if is_greedy_action:\n",
    "        #Random choice, if there are at least 2 Q-s with the same values\n",
    "        # TODO: Check, if randomizing the q_vector in lookup_action_value() around 0, what would happen\n",
    "        max_q_args = np.argwhere(q_vector == np.amax(q_vector))\n",
    "        if len(max_q_args) > 1:\n",
    "            action = np.random.choice(max_q_args.ravel(), 1)[0] + 1\n",
    "        else:\n",
    "            action = np.argmax(q_vector) + 1\n",
    "    else:\n",
    "        action = np.random.randint(1, 5)\n",
    "    return action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def td0_update_action_value(current_action_value, next_action_value, reward, alpha=0.25, gamma=0.95):\n",
    "    return current_action_value + alpha * (reward + gamma * next_action_value - current_action_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def td0_update(next_state, action_value_vect, action, reward, epsilon):\n",
    "    current = action_value_vect[action - 1]\n",
    "    n_action_value = lookup_action_value_wo_update(next_state)\n",
    "    n_action = epsilon_greedy_policy(n_action_value, epsilon)\n",
    "    new_q = td0_update_action_value(current, n_action_value[n_action - 1], reward)\n",
    "    action_value_vector[action - 1] = new_q\n",
    "    return action_value_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def random_policy():\n",
    "    return np.random.randint(1, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def action_value_update(q, discounted_reward, n):\n",
    "    return q + (discounted_reward - q) / n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def lookup_action_value(state):\n",
    "    if state not in action_values_table:\n",
    "        action_values_table[state] = [0, 0, 0, 0]\n",
    "    return action_values_table[state]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def lookup_action_value_wo_update(state):\n",
    "    av = action_values_table.get(state)\n",
    "    return av if av else [0, 0, 0, 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def try_table():\n",
    "    game = WindyGridWorld(GRID_SIZE, WINNER_TILE, WINDY_ARRAY, START_TILE)\n",
    "    current_pos = game.current_pos()\n",
    "    is_ended = False\n",
    "    agent_positions = deque()\n",
    "    states_reward_list = []\n",
    "    actions_deque = deque()\n",
    "    epsilon = 0\n",
    "    \n",
    "    while not is_ended:\n",
    "        \n",
    "        # Append current agent position\n",
    "        agent_positions.append(current_pos)\n",
    "\n",
    "        # Lookup action value belonging to current state\n",
    "        action_value_vector = lookup_action_value(current_pos)\n",
    "\n",
    "        # Choose action based on current action value vector and epsilon\n",
    "        action = epsilon_greedy_policy(action_value_vector, epsilon)\n",
    "        # Append action\n",
    "        actions_deque.append(action)\n",
    "\n",
    "        # Get next state, reward\n",
    "        current_pos, reward, is_ended = game.step(action)\n",
    "    \n",
    "    return agent_positions, actions_deque, current_pos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def show_moves(visited_states, grid_size):\n",
    "    arr = np.asarray(visited_states).T\n",
    "    range_x = (0.5, grid_size[1] + 0.5)\n",
    "    range_y = (0.5, grid_size[0] + 0.5)\n",
    "    ax = plt.gca()\n",
    "    ax.scatter(arr[1], arr[0])\n",
    "    ax.quiver(arr[1,:-1],arr[0,:-1],arr[1,1:]-arr[1,:-1],arr[0,1:]-arr[0,:-1], scale_units='xy', angles='xy', scale=1)\n",
    "    ax.set_xticks(np.arange(*range_x), minor=True)\n",
    "    ax.set_yticks(np.arange(*range_y), minor=True)\n",
    "    ax.set_xlim(*range_x)\n",
    "    ax.set_ylim(*range_y)\n",
    "    ax.set_xlabel(\"Valami\")\n",
    "    ax.invert_yaxis()\n",
    "    ax.get_xaxis().set_tick_params(labeltop=\"on\", labelbottom=\"off\")\n",
    "    plt.grid(which=\"minor\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Game parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "GRID_SIZE = (10, 10)\n",
    "WINNER_TILE = (1, 10)\n",
    "WINDY_ARRAY = (0, 1, 1, 2, -2, -1, -1, 1, 1, 0)\n",
    "START_TILE = (10, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "np.random.randint(-2, 3, 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Main learning loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "action_values_table = {}\n",
    "\n",
    "epsilon = 1\n",
    "episode_number = 0\n",
    "\n",
    "#Külső ciklus\n",
    "for _ in range(1000):\n",
    "    print(\"-\", end=\"\")\n",
    "    episode_number += 1\n",
    "    if episode_number % 100 == 0:\n",
    "        print(str(episode_number) + \". játék\")\n",
    "        print(\"Lépések száma: \" + str(len(actions_deque)))\n",
    "\n",
    "    epsilon = 1 / np.sqrt(episode_number)\n",
    "    # Containers\n",
    "    agent_positions = deque()\n",
    "    states_reward_list = []\n",
    "    actions_deque = deque()\n",
    "    # End bool\n",
    "    is_ended = False\n",
    "    # New game\n",
    "    game = WindyGridWorld(GRID_SIZE, WINNER_TILE, WINDY_ARRAY, START_TILE)\n",
    "    current_pos = game.current_pos()\n",
    "    #Belső ciklus\n",
    "    while not is_ended:\n",
    "        \n",
    "        # Append current agent position\n",
    "        agent_positions.append(current_pos)\n",
    "\n",
    "        # Lookup action value belonging to current state\n",
    "        action_value_vector = lookup_action_value(current_pos)\n",
    "\n",
    "        # Choose action based on current action value vector and epsilon\n",
    "        action = epsilon_greedy_policy(action_value_vector, epsilon)\n",
    "        # Append action\n",
    "        actions_deque.append(action)\n",
    "\n",
    "        # Get next state, reward\n",
    "        current_pos, reward, is_ended = game.step(action)\n",
    "\n",
    "        # Append reward\n",
    "        states_reward_list.append(reward)\n",
    "        \n",
    "        # Update action value\n",
    "        action_values_table[agent_positions[-1]] = td0_update(current_pos, action_value_vector,\n",
    "                                                                 action, reward, epsilon)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "a, b, c = try_table()\n",
    "a.append(c)\n",
    "show_moves(a, GRID_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "WINDY_ARRAY"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:tensorflow]",
   "language": "python",
   "name": "conda-env-tensorflow-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
