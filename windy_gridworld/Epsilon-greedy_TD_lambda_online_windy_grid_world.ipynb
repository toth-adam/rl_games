{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from windy import WindyGridWorld\n",
    "import numpy as np\n",
    "from collections import deque\n",
    "import pickle\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Necessary functions "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def epsilon_greedy_policy(q_vector, epsilon):\n",
    "    is_greedy_action = False if np.random.uniform() <= epsilon else True\n",
    "    if is_greedy_action:\n",
    "        #Random choice, if there are at least 2 Q-s with the same values\n",
    "        # TODO: Check, if randomizing the q_vector in lookup_action_value() around 0, what would happen\n",
    "        max_q_args = np.argwhere(q_vector == np.amax(q_vector))\n",
    "        if len(max_q_args) > 1:\n",
    "            action = np.random.choice(max_q_args.ravel(), 1)[0] + 1\n",
    "        else:\n",
    "            action = np.argmax(q_vector) + 1\n",
    "    else:\n",
    "        action = np.random.randint(1, 9)\n",
    "    return action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def td_error(c_a_v, n_a_v, r, g=0.95):\n",
    "    return r + g * n_a_v - c_a_v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def td_lambda_update(curr_state, next_state, action_value_vect, action, reward, epsilon, _lambda=0.9, gamma=0.95, alpha=0.15):\n",
    "    # Get current action-value (belonging to current state and action)\n",
    "    current_action_value = action_value_vect[action - 1]\n",
    "    # Get the next state action-value vector\n",
    "    next_action_value_vector = lookup_action_value(next_state)\n",
    "    # Get the next action according to the next_action_value_vector and epsilon(Using epsilon-greedy policy with current epsilon)\n",
    "    next_action = epsilon_greedy_policy(next_action_value_vector, epsilon)\n",
    "    # Get next action-value from the next_action_value_vector according our action\n",
    "    next_action_value = next_action_value_vector[next_action - 1]\n",
    "    \n",
    "    ## UPDATE ACTION_VALUES_TABLE ##\n",
    "    \n",
    "    # Calculate the TD-error\n",
    "    delta = td_error(current_action_value, next_action_value, reward, gamma)\n",
    "    # Update eligibility trace for current state and action\n",
    "    ACTION_VALUES_TABLE[curr_state][1, action - 1] += 1\n",
    "    \n",
    "    # Update all state in ACTION_VALUES_TABLE\n",
    "    for val in ACTION_VALUES_TABLE.values():\n",
    "        # Update action-value\n",
    "        val[0] += alpha * delta * val[1]\n",
    "        # Update eligibility traces \n",
    "        val[1] *= gamma * _lambda\n",
    "    return next_action, next_action_value_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def lookup_action_value(state):\n",
    "    if state not in ACTION_VALUES_TABLE:\n",
    "        ACTION_VALUES_TABLE[state] = np.zeros((2, 8), dtype=np.float64)\n",
    "    return ACTION_VALUES_TABLE[state][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def try_table():\n",
    "    game = WindyGridWorld(GRID_SIZE, WINNER_TILE, WINDY_ARRAY, START_TILE)\n",
    "    current_pos = game.current_pos()\n",
    "    is_ended = False\n",
    "    agent_positions = deque()\n",
    "    states_reward_list = []\n",
    "    actions_deque = deque()\n",
    "    epsilon = 0.00001\n",
    "    \n",
    "    while not is_ended:\n",
    "        \n",
    "        # Append current agent position\n",
    "        agent_positions.append(current_pos)\n",
    "\n",
    "        # Lookup action value belonging to current state\n",
    "        action_value_vector = lookup_action_value(current_pos)\n",
    "\n",
    "        # Choose action based on current action value vector and epsilon\n",
    "        action = epsilon_greedy_policy(action_value_vector, epsilon)\n",
    "        # Append action\n",
    "        actions_deque.append(action)\n",
    "\n",
    "        # Get next state, reward\n",
    "        current_pos, reward, is_ended = game.step(action)\n",
    "    \n",
    "    return agent_positions, actions_deque, current_pos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def show_moves(visited_states, grid_size):\n",
    "    arr = np.asarray(visited_states).T\n",
    "    range_x = (0.5, grid_size[1] + 0.5)\n",
    "    range_y = (0.5, grid_size[0] + 0.5)\n",
    "    ax = plt.gca()\n",
    "    ax.scatter(arr[1], arr[0])\n",
    "    ax.quiver(arr[1,:-1],arr[0,:-1],arr[1,1:]-arr[1,:-1],arr[0,1:]-arr[0,:-1], scale_units='xy', angles='xy', scale=1)\n",
    "    ax.set_xticks(np.arange(*range_x), minor=True)\n",
    "    ax.set_yticks(np.arange(*range_y), minor=True)\n",
    "    ax.set_xlim(*range_x)\n",
    "    ax.set_ylim(*range_y)\n",
    "    ax.set_xlabel(\"Valami\")\n",
    "    ax.invert_yaxis()\n",
    "    ax.get_xaxis().set_tick_params(labeltop=\"on\", labelbottom=\"off\")\n",
    "    ax.set_aspect(\"equal\")\n",
    "    plt.grid(which=\"minor\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Game parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "GRID_SIZE = (20, 20)\n",
    "WINNER_TILE = (15, 20)\n",
    "# WINDY_ARRAY = (0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 2, 2, 0)\n",
    "WINDY_ARRAY = np.zeros(20)\n",
    "START_TILE = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Main learning loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "ACTION_VALUES_TABLE = {}\n",
    "\n",
    "epsilon = 1\n",
    "episode_number = 0\n",
    "\n",
    "average = 0\n",
    "alpha = 0.15\n",
    "episodes_number=20000\n",
    "#Külső ciklus\n",
    "for curr_episode in range(episodes_number):\n",
    "    print(\"-\", end=\"\")\n",
    "    episode_number += 1\n",
    "    alpha = alpha - alpha/episodes_number* curr_episode\n",
    "    if episode_number % 1000 == 0:\n",
    "        print(str(episode_number) + \". játék\")\n",
    "        print(\"Lépések száma: \" + str(average / 1000))\n",
    "        average = 0\n",
    "\n",
    "    epsilon = 1 / (episode_number**(1/3))\n",
    "    # Containers\n",
    "    agent_positions = deque()\n",
    "    states_reward_list = []\n",
    "    actions_deque = deque()\n",
    "    # End bool\n",
    "    is_ended = False\n",
    "    \n",
    "    # Reset eligibility traces to zero\n",
    "    for val in ACTION_VALUES_TABLE.values():\n",
    "        val[1] = np.zeros_like(val[1])\n",
    "    \n",
    "    # New game\n",
    "    game = WindyGridWorld(GRID_SIZE, WINNER_TILE, WINDY_ARRAY, START_TILE)\n",
    "    current_pos = game.current_pos()\n",
    "    # Lookup action value belonging to current state\n",
    "    action_value_vector = lookup_action_value(current_pos)\n",
    "    # Choose action based on current action value vector and epsilon\n",
    "    action = epsilon_greedy_policy(action_value_vector, epsilon)\n",
    "    \n",
    "    #Belső ciklus\n",
    "    while not is_ended:\n",
    "        \n",
    "        # Append current agent position\n",
    "        agent_positions.append(current_pos)\n",
    "\n",
    "        # Append action\n",
    "        actions_deque.append(action)\n",
    "\n",
    "        # Get next state, reward\n",
    "        current_pos, reward, is_ended = game.step(action)\n",
    "\n",
    "        # Append reward\n",
    "        states_reward_list.append(reward)\n",
    "        \n",
    "        # TD lambda update (SIDE-EFFECt: Update ACTION_VALUES_TABLE)\n",
    "        action, action_value_vector = td_lambda_update(agent_positions[-1], current_pos, action_value_vector, action, reward, epsilon)\n",
    "    \n",
    "    # TODO: Something is wrong with this, dunno what yet\n",
    "    vertical_distance = WINNER_TILE[0] - agent_positions[0][0]\n",
    "    horizontal_distance = WINNER_TILE[1] - agent_positions[0][1]\n",
    "    optimal = max((horizontal_distance, vertical_distance))\n",
    "    average += len(actions_deque) / optimal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "a, b, c = try_table()\n",
    "a.append(c)\n",
    "show_moves(a, GRID_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "count = 0\n",
    "for _ in range(4000):\n",
    "    a,b,c = try_table()\n",
    "    count += len(b) / 19\n",
    "count / 4000"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:tensorflow]",
   "language": "python",
   "name": "conda-env-tensorflow-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
