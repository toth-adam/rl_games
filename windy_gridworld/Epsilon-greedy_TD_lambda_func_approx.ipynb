{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from windy import WindyGridWorld\n",
    "import numpy as np\n",
    "from collections import deque\n",
    "import pickle\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Necessary functions "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def epsilon_greedy_policy(q_vector, epsilon):\n",
    "    is_greedy_action = False if np.random.uniform() <= epsilon else True\n",
    "    if is_greedy_action:\n",
    "        #Random choice, if there are at least 2 Q-s with the same values\n",
    "        # TODO: Check, if randomizing the q_vector in lookup_action_value() around 0, what would happen\n",
    "        max_q_args = np.argwhere(q_vector == np.amax(q_vector))\n",
    "        if len(max_q_args) > 1:\n",
    "            action = np.random.choice(max_q_args.ravel(), 1)[0] + 1\n",
    "        else:\n",
    "            action = np.argmax(q_vector) + 1\n",
    "    else:\n",
    "        action = np.random.randint(1, 5)\n",
    "    return action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def td_error(c_a_v, n_a_v, r, g=0.95):\n",
    "    return r + g * n_a_v - c_a_v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def action_value(feature_vector, weights):\n",
    "    return feature_vector @ weights.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def get_action_value_vector(state, weights):\n",
    "    val = []\n",
    "    for act in range(1, 5):\n",
    "        val.append(get_action_value(state, weights, act))\n",
    "    return val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def calc_distance(state_vector, winner_tile):\n",
    "    return np.linalg.norm(np.asarray(state_vector)-np.asarray(winner_tile))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def feature_vector(state_vector, action):\n",
    "    return np.hstack((1 / (np.asarray(state_vector) + (np.asarray([-1, 0]), np.asarray([0, 1]), np.asarray([1, 0]), np.asarray([0, -1]))[action - 1] + np.array([1, 1])), 1 / np.asarray(state_vector)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_action_value(s, w, a):\n",
    "    return action_value(feature_vector(s, a), w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def td_lambda_update(curr_state, next_state, action, w, e_trace, reward, epsilon, _lambda=0.9, gamma=0.95, alpha=0.15):\n",
    "    # Get current action-value (belonging to current state and action)\n",
    "    current_action_value = get_action_value(curr_state, w, action)\n",
    "    # Get the next state action-value vector\n",
    "    next_action_value_vector = get_action_value_vector(next_state, w)\n",
    "    # Get the next action according to the next_action_value_vector and epsilon(Using epsilon-greedy policy with current epsilon)\n",
    "    next_action = epsilon_greedy_policy(next_action_value_vector, epsilon)\n",
    "    # Get next action-value from the next_action_value_vector according our action\n",
    "    next_action_value = next_action_value_vector[next_action - 1]\n",
    "    \n",
    "    ## UPDATE THE WEIGHTS ##\n",
    "    \n",
    "    # Calculate the TD-error\n",
    "    delta = td_error(current_action_value, next_action_value, reward, gamma)\n",
    "    # Update eligibility trace\n",
    "    new_e_trace = gamma * _lambda * e_trace + feature_vector(curr_state, action)\n",
    "    # Weights delta\n",
    "    delta_w = alpha * delta * new_e_trace\n",
    "    \n",
    "    return next_action, delta_w, new_e_trace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def try_table(w):\n",
    "    game = WindyGridWorld(GRID_SIZE, WINNER_TILE, WINDY_ARRAY, START_TILE)\n",
    "    current_pos = game.current_pos()\n",
    "    is_ended = False\n",
    "    agent_positions = deque()\n",
    "    states_reward_list = []\n",
    "    actions_deque = deque()\n",
    "    epsilon = 0.01\n",
    "    \n",
    "    while not is_ended:\n",
    "        \n",
    "        # Append current agent position\n",
    "        agent_positions.append(current_pos)\n",
    "\n",
    "        # Lookup action value belonging to current state\n",
    "        action_value_vector = get_action_value_vector(current_pos, w)\n",
    "\n",
    "        # Choose action based on current action value vector and epsilon\n",
    "        action = epsilon_greedy_policy(action_value_vector, epsilon)\n",
    "        # Append action\n",
    "        actions_deque.append(action)\n",
    "\n",
    "        # Get next state, reward\n",
    "        current_pos, reward, is_ended = game.step(action)\n",
    "    \n",
    "    return agent_positions, actions_deque, current_pos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def show_moves(visited_states, grid_size):\n",
    "    arr = np.asarray(visited_states).T\n",
    "    range_x = (0.5, grid_size[1] + 0.5)\n",
    "    range_y = (0.5, grid_size[0] + 0.5)\n",
    "    ax = plt.gca()\n",
    "    ax.scatter(arr[1], arr[0])\n",
    "    ax.quiver(arr[1,:-1],arr[0,:-1],arr[1,1:]-arr[1,:-1],arr[0,1:]-arr[0,:-1], scale_units='xy', angles='xy', scale=1)\n",
    "    ax.set_xticks(np.arange(*range_x), minor=True)\n",
    "    ax.set_yticks(np.arange(*range_y), minor=True)\n",
    "    ax.set_xlim(*range_x)\n",
    "    ax.set_ylim(*range_y)\n",
    "    ax.set_xlabel(\"Valami\")\n",
    "    ax.invert_yaxis()\n",
    "    ax.get_xaxis().set_tick_params(labeltop=\"on\", labelbottom=\"off\")\n",
    "    ax.set_aspect(\"equal\")\n",
    "    plt.grid(which=\"minor\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Game parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "GRID_SIZE = (6, 6)\n",
    "WINNER_TILE = (3, 6)\n",
    "# WINDY_ARRAY = (0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 2, 2, 0)\n",
    "WINDY_ARRAY = np.zeros(6)\n",
    "START_TILE = (1, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Main learning loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "feature_vector_size = 4\n",
    "W = np.zeros(feature_vector_size)\n",
    "\n",
    "epsilon = 1\n",
    "episode_number = 0\n",
    "\n",
    "average = 0\n",
    "alpha = 0.15\n",
    "episodes_number=200\n",
    "#Külső ciklus\n",
    "for curr_episode in range(episodes_number):\n",
    "    print(\"-\", end=\"\")\n",
    "    episode_number += 1\n",
    "    alpha = alpha - alpha/episodes_number* curr_episode\n",
    "    if episode_number % 1000 == 0:\n",
    "        print(str(episode_number) + \". játék\")\n",
    "        print(\"Lépések száma: \" + str(average / 1000))\n",
    "        average = 0\n",
    "\n",
    "    epsilon = 1 / (episode_number**(1/3))\n",
    "    # Containers\n",
    "    agent_positions = deque()\n",
    "    states_reward_list = []\n",
    "    actions_deque = deque()\n",
    "    # End bool\n",
    "    is_ended = False\n",
    "    \n",
    "    # Reset eligibility traces to zero\n",
    "    ELIGIBILITY_TRACE = np.zeros(feature_vector_size)\n",
    "    \n",
    "    # New game\n",
    "    game = WindyGridWorld(GRID_SIZE, WINNER_TILE, WINDY_ARRAY, START_TILE)\n",
    "    current_pos = game.current_pos()\n",
    "    # TODO: check if current_pos type (tuple) is a problem or not\n",
    "    # Get action value belonging to current state (through feature vectors)\n",
    "    action_value_vector = get_action_value_vector(current_pos, W)\n",
    "    # Choose action based on current action value vector and epsilon\n",
    "    action = epsilon_greedy_policy(action_value_vector, epsilon)\n",
    "    \n",
    "    #Belső ciklus\n",
    "    while not is_ended:\n",
    "    #for _ in range(15):\n",
    "        \n",
    "        # Append current agent position\n",
    "        agent_positions.append(current_pos)\n",
    "\n",
    "        # Append action\n",
    "        actions_deque.append(action)\n",
    "\n",
    "        # Get next state, reward\n",
    "        current_pos, reward, is_ended = game.step(action)\n",
    "\n",
    "        # Append reward\n",
    "        states_reward_list.append(reward)\n",
    "        \n",
    "        # TD lambda update\n",
    "        action, delta_W, ELIGIBILITY_TRACE = td_lambda_update(agent_positions[-1], current_pos, action, W, ELIGIBILITY_TRACE, reward, epsilon)\n",
    "        W += delta_W\n",
    "    \n",
    "    # TODO: Something is wrong with this, dunno what yet\n",
    "    vertical_distance = WINNER_TILE[0] - agent_positions[0][0]\n",
    "    horizontal_distance = WINNER_TILE[1] - agent_positions[0][1]\n",
    "    optimal = max((horizontal_distance, vertical_distance))\n",
    "    average += len(actions_deque) / optimal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "get_action_value_vector((2, 6), W)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "a, b, c = try_table(W)\n",
    "a.append(c)\n",
    "show_moves(a, GRID_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "count = 0\n",
    "for _ in range(4000):\n",
    "    a,b,c = try_table()\n",
    "    count += len(b) / 19\n",
    "count / 4000"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:tensorflow]",
   "language": "python",
   "name": "conda-env-tensorflow-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
