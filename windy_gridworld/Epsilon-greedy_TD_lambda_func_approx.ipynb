{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from windy import WindyGridWorld\n",
    "import numpy as np\n",
    "from collections import deque\n",
    "import pickle\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Necessary functions "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def epsilon_greedy_policy(q_vector, epsilon):\n",
    "    is_greedy_action = False if np.random.uniform() <= epsilon else True\n",
    "    if is_greedy_action:\n",
    "        #Random choice, if there are at least 2 Q-s with the same values\n",
    "        # TODO: Check, if randomizing the q_vector in lookup_action_value() around 0, what would happen\n",
    "        max_q_args = np.argwhere(q_vector == np.amax(q_vector))\n",
    "        if len(max_q_args) > 1:\n",
    "            action = np.random.choice(max_q_args.ravel(), 1)[0] + 1\n",
    "        else:\n",
    "            action = np.argmax(q_vector) + 1\n",
    "    else:\n",
    "        action = np.random.randint(1, 9)\n",
    "    return action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def td_error(c_a_v, n_a_v, r, g=0.95):\n",
    "    return r + g * n_a_v - c_a_v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def action_value(feature_vector, weights):\n",
    "    return feature_vector @ weights.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def get_action_value_vector(state, weights):\n",
    "    val = []\n",
    "    for act in np.eye(8):\n",
    "        f = feature_vector(state, act)\n",
    "        val.append(action_value(f, weights))\n",
    "    return val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_action_vector(action):\n",
    "    return np.eye(8)[action - 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def feature_vector(state_vector, action_vector):\n",
    "    return np.hstack((state_vector, action_vector))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_action_value(s, w, a):\n",
    "    return action_value(feature_vector(s, get_action_vector(a)), w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def td_lambda_update(curr_state, next_state, action, w, e_trace, reward, epsilon, _lambda=0.9, gamma=0.95, alpha=0.15):\n",
    "    # Get current action-value (belonging to current state and action)\n",
    "    current_action_value = get_action_value(curr_state, w, action)\n",
    "    # Get the next state action-value vector\n",
    "    next_action_value_vector = get_action_value_vector(next_state, w)\n",
    "    # Get the next action according to the next_action_value_vector and epsilon(Using epsilon-greedy policy with current epsilon)\n",
    "    next_action = epsilon_greedy_policy(next_action_value_vector, epsilon)\n",
    "    # Get next action-value from the next_action_value_vector according our action\n",
    "    next_action_value = next_action_value_vector[next_action - 1]\n",
    "    \n",
    "    ## UPDATE THE WEIGHTS ##\n",
    "    \n",
    "    # Calculate the TD-error\n",
    "    delta = td_error(current_action_value, next_action_value, reward, gamma)\n",
    "    # Update eligibility trace\n",
    "    new_e_trace = gamma * _lambda * e_trace + feature_vector(curr_state, get_action_vector(action))\n",
    "    # Weights delta\n",
    "    delta_w = alpha * delta * new_e_trace\n",
    "    \n",
    "    return next_action, delta_w, new_e_trace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def try_table(w):\n",
    "    game = WindyGridWorld(GRID_SIZE, WINNER_TILE, WINDY_ARRAY, START_TILE)\n",
    "    current_pos = game.current_pos()\n",
    "    is_ended = False\n",
    "    agent_positions = deque()\n",
    "    states_reward_list = []\n",
    "    actions_deque = deque()\n",
    "    epsilon = 0.01\n",
    "    \n",
    "    while not is_ended:\n",
    "        \n",
    "        # Append current agent position\n",
    "        agent_positions.append(current_pos)\n",
    "\n",
    "        # Lookup action value belonging to current state\n",
    "        action_value_vector = get_action_value_vector(current_pos, w)\n",
    "\n",
    "        # Choose action based on current action value vector and epsilon\n",
    "        action = epsilon_greedy_policy(action_value_vector, epsilon)\n",
    "        # Append action\n",
    "        actions_deque.append(action)\n",
    "\n",
    "        # Get next state, reward\n",
    "        current_pos, reward, is_ended = game.step(action)\n",
    "    \n",
    "    return agent_positions, actions_deque, current_pos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def show_moves(visited_states, grid_size):\n",
    "    arr = np.asarray(visited_states).T\n",
    "    range_x = (0.5, grid_size[1] + 0.5)\n",
    "    range_y = (0.5, grid_size[0] + 0.5)\n",
    "    ax = plt.gca()\n",
    "    ax.scatter(arr[1], arr[0])\n",
    "    ax.quiver(arr[1,:-1],arr[0,:-1],arr[1,1:]-arr[1,:-1],arr[0,1:]-arr[0,:-1], scale_units='xy', angles='xy', scale=1)\n",
    "    ax.set_xticks(np.arange(*range_x), minor=True)\n",
    "    ax.set_yticks(np.arange(*range_y), minor=True)\n",
    "    ax.set_xlim(*range_x)\n",
    "    ax.set_ylim(*range_y)\n",
    "    ax.set_xlabel(\"Valami\")\n",
    "    ax.invert_yaxis()\n",
    "    ax.get_xaxis().set_tick_params(labeltop=\"on\", labelbottom=\"off\")\n",
    "    ax.set_aspect(\"equal\")\n",
    "    plt.grid(which=\"minor\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Game parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "GRID_SIZE = (6, 6)\n",
    "WINNER_TILE = (3, 6)\n",
    "# WINDY_ARRAY = (0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 2, 2, 0)\n",
    "WINDY_ARRAY = np.zeros(6)\n",
    "START_TILE = (1, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Main learning loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----"
     ]
    }
   ],
   "source": [
    "feature_vector_size = 10\n",
    "W = np.zeros(feature_vector_size)\n",
    "\n",
    "epsilon = 1\n",
    "episode_number = 0\n",
    "\n",
    "average = 0\n",
    "alpha = 0.15\n",
    "episodes_number=5\n",
    "#Külső ciklus\n",
    "for curr_episode in range(episodes_number):\n",
    "    print(\"-\", end=\"\")\n",
    "    episode_number += 1\n",
    "    alpha = alpha - alpha/episodes_number* curr_episode\n",
    "    if episode_number % 1000 == 0:\n",
    "        print(str(episode_number) + \". játék\")\n",
    "        print(\"Lépések száma: \" + str(average / 1000))\n",
    "        average = 0\n",
    "\n",
    "    epsilon = 1 / (episode_number**(1/3))\n",
    "    # Containers\n",
    "    agent_positions = deque()\n",
    "    states_reward_list = []\n",
    "    actions_deque = deque()\n",
    "    # End bool\n",
    "    is_ended = False\n",
    "    \n",
    "    # Reset eligibility traces to zero\n",
    "    ELIGIBILITY_TRACE = np.zeros(feature_vector_size)\n",
    "    \n",
    "    # New game\n",
    "    game = WindyGridWorld(GRID_SIZE, WINNER_TILE, WINDY_ARRAY, START_TILE)\n",
    "    current_pos = game.current_pos()\n",
    "    # TODO: check if current_pos type (tuple) is a problem or not\n",
    "    # Get action value belonging to current state (through feature vectors)\n",
    "    action_value_vector = get_action_value_vector(current_pos, W)\n",
    "    # Choose action based on current action value vector and epsilon\n",
    "    action = epsilon_greedy_policy(action_value_vector, epsilon)\n",
    "    \n",
    "    #Belső ciklus\n",
    "    while not is_ended:\n",
    "        \n",
    "        # Append current agent position\n",
    "        agent_positions.append(current_pos)\n",
    "\n",
    "        # Append action\n",
    "        actions_deque.append(action)\n",
    "\n",
    "        # Get next state, reward\n",
    "        current_pos, reward, is_ended = game.step(action)\n",
    "\n",
    "        # Append reward\n",
    "        states_reward_list.append(reward)\n",
    "        \n",
    "        # TD lambda update\n",
    "        action, delta_W, ELIGIBILITY_TRACE = td_lambda_update(agent_positions[-1], current_pos, action, W, ELIGIBILITY_TRACE, reward, epsilon)\n",
    "        W += delta_W\n",
    "    \n",
    "    # TODO: Something is wrong with this, dunno what yet\n",
    "    vertical_distance = WINNER_TILE[0] - agent_positions[0][0]\n",
    "    horizontal_distance = WINNER_TILE[1] - agent_positions[0][1]\n",
    "    optimal = max((horizontal_distance, vertical_distance))\n",
    "    average += len(actions_deque) / optimal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ -3.23683770e+136,  -7.63034110e+136,  -3.62860044e+135,\n",
       "        -4.26592057e+134,  -2.34315708e+135,  -2.13024904e+135,\n",
       "        -3.47699142e+135,  -2.56870694e+135,  -1.29123853e+135,\n",
       "        -1.07539039e+135])"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "W"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 12.03700657,  32.64897825,   1.96997813,   0.11804845,\n",
       "         1.40855199,   0.4257001 ,   1.32790757,   0.97973667,\n",
       "         0.35844208,   0.30818674])"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ELIGIBILITY_TRACE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-33-6313d02d098c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtry_table\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mW\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0ma\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mshow_moves\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mGRID_SIZE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-32-4ca53e25affd>\u001b[0m in \u001b[0;36mtry_table\u001b[0;34m(w)\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m         \u001b[0;31m# Lookup action value belonging to current state\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m         \u001b[0maction_value_vector\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_action_value_vector\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcurrent_pos\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m         \u001b[0;31m# Choose action based on current action value vector and epsilon\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-19-fcdcf9ab8b68>\u001b[0m in \u001b[0;36mget_action_value_vector\u001b[0;34m(state, weights)\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0mval\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mact\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meye\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m8\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m         \u001b[0mf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfeature_vector\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mact\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m         \u001b[0mval\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction_value\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweights\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mval\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-21-2577ad7f89e1>\u001b[0m in \u001b[0;36mfeature_vector\u001b[0;34m(state_vector, action_vector)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mfeature_vector\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate_vector\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maction_vector\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate_vector\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maction_vector\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/home/atoth/miniconda3/envs/tensorflow/lib/python3.5/site-packages/numpy/core/shape_base.py\u001b[0m in \u001b[0;36mhstack\u001b[0;34m(tup)\u001b[0m\n\u001b[1;32m    273\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    274\u001b[0m     \"\"\"\n\u001b[0;32m--> 275\u001b[0;31m     \u001b[0marrs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0matleast_1d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_m\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0m_m\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtup\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    276\u001b[0m     \u001b[0;31m# As a special case, dimension 0 of 1-dimensional arrays is \"horizontal\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    277\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0marrs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndim\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/atoth/miniconda3/envs/tensorflow/lib/python3.5/site-packages/numpy/core/shape_base.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    273\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    274\u001b[0m     \"\"\"\n\u001b[0;32m--> 275\u001b[0;31m     \u001b[0marrs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0matleast_1d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_m\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0m_m\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtup\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    276\u001b[0m     \u001b[0;31m# As a special case, dimension 0 of 1-dimensional arrays is \"horizontal\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    277\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0marrs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndim\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/atoth/miniconda3/envs/tensorflow/lib/python3.5/site-packages/numpy/core/shape_base.py\u001b[0m in \u001b[0;36matleast_1d\u001b[0;34m(*arys)\u001b[0m\n\u001b[1;32m     47\u001b[0m     \"\"\"\n\u001b[1;32m     48\u001b[0m     \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 49\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0mary\u001b[0m \u001b[0;32min\u001b[0m \u001b[0marys\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     50\u001b[0m         \u001b[0mary\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0masanyarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mary\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mary\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "a, b, c = try_table(W)\n",
    "a.append(c)\n",
    "show_moves(a, GRID_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "count = 0\n",
    "for _ in range(4000):\n",
    "    a,b,c = try_table()\n",
    "    count += len(b) / 19\n",
    "count / 4000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ACTION_VALUES_TABLE[(15, 12)][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:tensorflow]",
   "language": "python",
   "name": "conda-env-tensorflow-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
