{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from twenty_forty_eight_linux import TwentyFortyEight\n",
    "from collections import deque\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Policy neural network hyperparameters\n",
    "INPUT_DIM = 16\n",
    "HIDDEN_LAYER_UNITS = 300\n",
    "OUTPUT_DIM = 4\n",
    "# Value function neural network hyperparameters\n",
    "VF_HIDDEN_LAYER_UNITS = 200\n",
    "VF_OUTPUT_DIM = 1\n",
    "# RMSProp hyperparameters (Policy)\n",
    "LEARNING_RATE = 0.0003\n",
    "DECAY_FACTOR = 0.9\n",
    "# RMSProp hyperparameters (Value function)\n",
    "VF_LEARNING_RATE = 0.001\n",
    "VF_DECAY_FACTOR = 0.9\n",
    "# RL hyperparameters\n",
    "DISCOUNT_FACTOR = 0.95\n",
    "# Loss hyperparameters\n",
    "ENTROPY_REGULARIZATION_FACTOR = 0.01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Game constants\n",
    "POSSIBLE_ACTIONS = np.arange(1, 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sess = tf.InteractiveSession()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Graph building"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Graph building helpers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Xavier initialization is harcoded just yet.\n",
    "\n",
    "def new_layer(input_tf_tensor, hidden_unit_number, scope_name, activation=tf.nn.relu, bias=True):\n",
    "    input_dimension = input_tf_tensor.get_shape().as_list()[1]\n",
    "    # Weight\n",
    "    with tf.variable_scope(scope_name):\n",
    "        W = tf.get_variable(\"W\", shape=(input_dimension, hidden_unit_number),\n",
    "                            initializer=tf.contrib.layers.xavier_initializer(False))\n",
    "        # Matrix multiplication\n",
    "        h = tf.matmul(input_tf_tensor, W)\n",
    "        if bias:\n",
    "            B = tf.get_variable(\"B\", shape=(1, hidden_unit_number),\n",
    "                                initializer=tf.contrib.layers.xavier_initializer(False))\n",
    "            hb = h + B\n",
    "    output = activation(hb) if bias else activation(h)\n",
    "    return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Input & direction placeholders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Policy network\n",
    "state = tf.placeholder(tf.float32, shape=[None, INPUT_DIM], name=\"state_tensor\")\n",
    "direction = tf.placeholder(tf.float32, shape=[None, OUTPUT_DIM], name=\"direction_label\")\n",
    "advantage_value = tf.placeholder(tf.float32, shape=[None, 1], name=\"advantage_value\")\n",
    "# Value function network (+state)\n",
    "reward = tf.placeholder(tf.float32, shape=[None, 1], name=\"reward\")\n",
    "prev_state_val = tf.placeholder(tf.float32, shape=(), name=\"previous_state_value\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Neural network operations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "policy_h1 = new_layer(state, HIDDEN_LAYER_UNITS, \"Policy_Hidden_1\")\n",
    "policy_h2 = new_layer(policy_h1, HIDDEN_LAYER_UNITS, \"Policy_Hidden_2\")\n",
    "policy_output = new_layer(policy_h2, OUTPUT_DIM, \"Policy_Output\", tf.nn.softmax)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Value function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "vf_h1 = new_layer(state, VF_HIDDEN_LAYER_UNITS, \"VF_Hidden_1\")\n",
    "vf_output = new_layer(vf_h1, VF_OUTPUT_DIM, \"VF_Output\", tf.reduce_sum)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Loss calculation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "policy_loss = - tf.reduce_sum(tf.log(tf.reduce_sum(policy_output * direction)) * advantage_value)\n",
    "policy_loss_function = tf.summary.scalar(\"loss_func\", policy_loss) # Summary op for TensorBoard"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Entropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "entropy = - tf.reduce_sum(policy_output * tf.log(policy_output))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Error calculation (ROSSZ, MERT NEM A RÉGI PARAMÉTERREL SZÁMOL A VF_OUTPUT-NÁL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "vf_error = tf.subtract(reward + DISCOUNT_FACTOR * vf_output, prev_state_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Value function loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "vf_loss = 0.5 * tf.square(vf_error)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Total loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "total_loss = policy_loss - entropy * ENTROPY_REGULARIZATION_FACTOR"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RMSPropOptimizer (Policy & value function)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_opt = tf.train.RMSPropOptimizer(LEARNING_RATE, decay=DECAY_FACTOR)\n",
    "vf_train_opt = tf.train.RMSPropOptimizer(VF_LEARNING_RATE, decay=VF_DECAY_FACTOR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradient calculation (Policy & value function)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Policy (we use negative loss, since we want gradient ASCENT)\n",
    "train_apply_grad = train_opt.minimize(total_loss)\n",
    "# Value function Neural network\n",
    "vf_apply_grad = vf_train_opt.minimize(vf_loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tf.global_variables_initializer().run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### FileWriter for TensorBoard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "summary = tf.summary.FileWriter(\"c:\\\\Work\\\\Coding\\\\Tensorflow_log\\\\2048\", sess.graph)\n",
    "merged = tf.summary.merge_all() # Merge all summary operations (In this case we only have loss_func)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Game "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def initialize_game():\n",
    "    return TwentyFortyEight(4, 4)\n",
    "\n",
    "def game_state(g):\n",
    "    return np.asarray(g.table_as_array(), dtype=np.float32).reshape(1, 16)\n",
    "\n",
    "def direction_vector(action):\n",
    "    return np.eye(4, dtype=np.float32)[action - 1].reshape(1, 4)\n",
    "\n",
    "def discounted_rewards(r):\n",
    "    gamma_vector = (DISCOUNT_FACTOR ** np.arange(len(r)))[::-1]\n",
    "    rewards = np.asarray(r, dtype=np.float32)\n",
    "    discounted = np.zeros_like(r, dtype=np.float32)\n",
    "    for i in range(len(r)):\n",
    "        discounted[i] =np.sum(rewards[i:] * gamma_vector[i:][::-1])\n",
    "    return discounted.reshape(len(r), 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ep_number = 0\n",
    "for _ in range(10):\n",
    "    # Initialize game\n",
    "    game = initialize_game()\n",
    "    states_input_deque, actions_deque, rewards_deque = deque(), deque(), deque()\n",
    "    is_ended = False\n",
    "    no_of_steps = 0\n",
    "    \n",
    "    current_state = game_state(game)\n",
    "    \n",
    "    while not is_ended:\n",
    "    #for step in range(5):\n",
    "        # Append current game state\n",
    "        states_input_deque.append(current_state)\n",
    "\n",
    "        # Choose action from the network and append it to the actions_deque\n",
    "        action_distribution = sess.run(policy_output, feed_dict={state: current_state})\n",
    "        action = np.random.choice(POSSIBLE_ACTIONS, 1, p=action_distribution.ravel())[0]\n",
    "        actions_deque.append(action)\n",
    "\n",
    "        # Make the move in the game\n",
    "        game.move(action)\n",
    "        no_of_steps += 1\n",
    "\n",
    "        # Get next state, reward\n",
    "        current_state, rew, is_ended = game_state(game), game.reward(), game.is_ended()\n",
    "        np_rew = np.asarray([[rew]])\n",
    "\n",
    "        # Append rewards\n",
    "        rewards_deque.append(rew)\n",
    "        \n",
    "        # State value\n",
    "        p_state_val = sess.run(vf_output, feed_dict={state: states_input_deque[-1]})\n",
    "        \n",
    "        # \"Advantage value\" calc\n",
    "        adv_val = sess.run(vf_error, feed_dict={reward: np_rew, prev_state_val: p_state_val, state: current_state})\n",
    "        \n",
    "        # Policy network parameter update\n",
    "        sess.run(train_apply_grad, feed_dict={state: states_input_deque[-1], direction: direction_vector(action), advantage_value: adv_val})\n",
    "        \n",
    "        \n",
    "        # Value function network parameter update (if we are not stuck in a state)\n",
    "        if not np.all(current_state == states_input_deque[-1]):\n",
    "            sess.run(vf_apply_grad, feed_dict={reward: np_rew, prev_state_val: p_state_val, state: current_state})\n",
    "        \n",
    "        # Checks\n",
    "        if (no_of_steps) % 500 == 0:\n",
    "            print(\"Step: \" + str(no_of_steps))\n",
    "            print(\"State: \" + str(states_input_deque[-1]))\n",
    "            print(\"Action distribtuion: \" + str(action_distribution))\n",
    "            print(\"Reward: \" + str(rew))\n",
    "            print(\"Previous state value: \"+ str(p_state_val))\n",
    "            print(\"Advantage value: \" + str(adv_val))\n",
    "    \n",
    "    print(\"--------------Episode over!----------------\")\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Flush out data to disk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "summary.flush()"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:tf]",
   "language": "python",
   "name": "conda-env-tf-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
